{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee2e9aaacfd00c7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# RAG-based Question Answering\n",
    "\n",
    "The exercise introduces modern approaches to Question Answering using Retrieval Augmented Generation (RAG) with LLMs and vector databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b3e7e06601866",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tasks\n",
    "\n",
    "Objectives (8 points):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4fef9d37d30315",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. Set up the QA environment:\n",
    "   * Install OLLAMA and select an appropriate LLM\n",
    "   * Configure [Qdrant](https://qdrant.tech/) vector database (or vector DB of your choosing)\n",
    "   * Install necessary Python packages for embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a11c4e7fa6c52423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:59:26.709180Z",
     "start_time": "2024-12-15T20:59:09.515700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "import time\n",
    "from datetime import datetime\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from typing import Dict, Tuple, List\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d00881f32f49a15c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:59:26.737892Z",
     "start_time": "2024-12-15T20:59:26.733068Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers qdrant-client\n",
    "#!pip install transformer\n",
    "#!pip install langchain pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73e4e001c91c8c09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:59:26.739860Z",
     "start_time": "2024-12-15T20:59:26.735399Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#docker run -p 127.0.0.1:6333:6333 qdrant/qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9f461f5c1a4d5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:59:27.589518Z",
     "start_time": "2024-12-15T20:59:26.736762Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!ollama --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "974bfb0fac382bf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:59:27.831263Z",
     "start_time": "2024-12-15T20:59:27.570650Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE      MODIFIED     \r\n",
      "mistral:latest     f974a74358d6    4.1 GB    3 hours ago     \r\n",
      "phi3:3.8b          4f2222927938    2.2 GB    11 days ago     \r\n",
      "starcoder2:3b      9f4ae0aff61e    1.7 GB    2 months ago    \r\n",
      "llama3.1:8b        42182419e950    4.7 GB    2 months ago    \r\n",
      "llama3.2:latest    a80c4f17acd5    2.0 GB    2 months ago    \r\n",
      "llama2:latest      78e26419b446    3.8 GB    8 months ago    \r\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e32273dc783180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:59:28.284403Z",
     "start_time": "2024-12-15T20:59:28.168877Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='cv_collection'), CollectionDescription(name='pdf_chunks')]\n"
     ]
    }
   ],
   "source": [
    "client = QdrantClient(url=\"http://127.0.0.1:6333\")\n",
    "print(client.get_collections())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7124ea13a846bedc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:59:32.476738Z",
     "start_time": "2024-12-15T20:59:28.289121Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 384\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "test_text = \"Przykładowy tekst.\"\n",
    "embedding = model.encode(test_text)\n",
    "\n",
    "print(\"Embedding size:\", len(embedding))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c208ddd774759b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Find PDF file of your choosing. Example - some publication or CV file:\n",
    "3. Write next procedures necessary for RAG pipeline. Use [LangChain](https://python.langchain.com/docs/introduction/) library:\n",
    " \n",
    "   * Load PDF file using `PyPDFLoader`.  \n",
    "   * Split documents into appropriate chunks using `RecursiveCharacterTextSplitter`.\n",
    "   * Generate and store embeddings in Qdrant database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b306aa09b730d591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:59:39.891432Z",
     "start_time": "2024-12-15T20:59:32.481311Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/xy853v6969d25m5d35x8gwj80000gn/T/ipykernel_15660/636242501.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "/var/folders/dh/xy853v6969d25m5d35x8gwj80000gn/T/ipykernel_15660/636242501.py:17: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"myfile.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "client = QdrantClient(url=\"http://127.0.0.1:6333\")\n",
    "collection_name = \"cv_collection\"\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "points = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    vector = embeddings.embed_query(chunk.page_content)\n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            vector=vector,\n",
    "            payload={\n",
    "                \"text\": chunk.page_content,\n",
    "                \"metadata\": dict(chunk.metadata) \n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de8f54657c8023",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. Design and implement the RAG pipeline with `LCEL`. As reference use this detailed guide created by LangChain community - [RAG](https://python.langchain.com/docs/tutorials/rag/). Next steps should involve:\n",
    "   * Create query embedding generation\n",
    "   * Implement semantic search in Qdrant\n",
    "   * Design prompt templates for context integration\n",
    "   * Build response generation with the LLM\n",
    "\n",
    "Hint: You don't need to build it from scratch. A lot of this steps is already automated using LCEL pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6e37754633d70f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T21:05:57.810163Z",
     "start_time": "2024-12-15T20:59:39.874165Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/xy853v6969d25m5d35x8gwj80000gn/T/ipykernel_15660/1107914587.py:1: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  model = ChatOllama(model=\"phi3:3.8b\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is re-ranking in the context of RAG?\n",
      "Answer: Re-ranking within a Retrieval Augmented Generation (RAG) system serves as an additional quality control mechanism that enhances the relevance and accuracy of retrieved responses. After performing an initial retrieval based on vector similarity, which provides the top-k most relevant answers to the user's query from a larger corpus or index, re-ranking is applied using further ranking criteria or contextual information. This process refines the list by prioritizing candidates that are more likely to be accurate and well-aligned with the intent of the original question. As such, it assists in ens05: What does BLEU score measure? Answer:\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: What are the main benefits of RAG?\n",
      "Answer: 1. Improved Accuracy: By leveraging a retrieval system in addition to language models, responses generated by RAG systems tend to be more accurate as they rely on relevant information from external databases or knowledge sources.\n",
      "2. Better Contextual Relevance: Since the initial search for matching documents and their context is done with these additional data points, it allows AI chatbots using this technique to provide answers that are highly relevant to user queries by understanding not just keywords but also sentence structure (syntax) within a query or text passage provided.\n",
      "3. Scalability: As the RAG approach uses precomputed representations of knowledge sources for fast lookup and retrieval, it supports handling more significant amounts of information than conventional AI systems without compromising on speed—making this technique suitable even as datasets grow in size exponentially over time.\n",
      "4 (implied): Reduced Hallucinations: RAG helps to limit instances where an LLM might generate a response with no backing from source material since it always relies partially on external knowledge sources alongside its own training data when generating text responses which, although not explicitly mentioned in the context provided above but can be inferred as crucial due to mentions of accuracy improvement associated primarily through this aspect.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: How does the Retrieval Augmented Generation process work?\n",
      "Answer: The RAG retrieval, or Retrieval Augmented Generation, is a two-step process that utilizes both dense vector representations and language models to generate accurate responses in an AI system. Initially, during the retrieval step, the user's query along with potential answers from documents or knowledge bases are encoded into dense vectors using various encoding techniques like BM25 or Sentence Transformer embeddings. Following that, a similarity search algorithm identifies and selects the top-k most relevant responses to the initial query based on these vector representations.\n",
      "\n",
      "The retrieved results then proceed to the generation step where they enter an encoder model such as T5_EncodePassage which is used in conjunction with large language models like GPT-3, BERT or RoBERTa for generating a refined answer that maintains context and relevance from within these top responses.\n",
      "\n",
      "Next up on the journey through AI systems are re-ranking techniques – powerful tools designed to enhance RAG retrieval system’s performance by fine-tuning initial search results before they generate final answers, thereby ensuring optimal accuracy and effectiveness of our 'AI librarian'.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: What re-ranking techniques are described in the file?\n",
      "Answer: The context provided does not describe specific re-ranking techniques but explains that there are additional ranking criteria or incorporation of contextual information used to refine and improve the quality of top-k candidates retrieved during initial search. The exact nature of these methods is not detailed in the given text, indicating I cannot find this particular detail about their mechanisms within the available context.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: What are examples of RAG applications?\n",
      "Answer: 1. Chatbots and Virtual Assistants: Enhancing accuracy in answering user queries with contextually relevant responses.\n",
      "2. Question Answering Systems: Providing accurate answers by retrieving the most pertinent information from extensive knowledge bases for open-domain questions.\n",
      "3. Content Recommendation and Personalization: Offering personalized content recommendations based on user preferences and interaction history to improve engagement.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ChatOllama(model=\"phi3:3.8b\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "client = QdrantClient(url=\"http://127.0.0.1:6333\")\n",
    "collection_name = \"cv_collection\"\n",
    "\n",
    "def get_relevant_documents(query: str) -> List[str]:\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=3  \n",
    "    )\n",
    "    if not results:\n",
    "        return [\"Brak wyników dla zapytania.\"]\n",
    "    return [hit.payload[\"text\"] for hit in results]\n",
    "\n",
    "prompt_template = \"\"\"Answer the question based ONLY on the context below. \n",
    "If you cannot find the answer in the context, say \"I cannot find the answer to this question in the available context.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "def format_docs(docs: List[str]) -> str:\n",
    "    return \"\\n\\n\".join(docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: format_docs(get_relevant_documents(x)),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def ask_question(question: str) -> str:\n",
    "    response = rag_chain.invoke(question)\n",
    "    return response\n",
    "\n",
    "test_questions = [\n",
    "    \"What is re-ranking in the context of RAG?\",\n",
    "    \"What are the main benefits of RAG?\",\n",
    "    \"How does the Retrieval Augmented Generation process work?\",\n",
    "    \"What re-ranking techniques are described in the file?\",\n",
    "    \"What are examples of RAG applications?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Answer: {ask_question(question)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4dfa91749e9e27",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5. Implement basic retrieval strategies (semantic search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb8be1c84f7be60d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:01:01.368505Z",
     "start_time": "2024-12-15T22:01:01.356971Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def semantic_search(query: str, limit: int = 3) -> List[str]:\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=limit\n",
    "    )\n",
    "    return [hit.payload[\"text\"] for hit in results] if results else [\"No relevant documents found.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5613607b5f6973cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:01:02.435437Z",
     "start_time": "2024-12-15T22:01:02.296181Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the main components of a RAG system?\n",
      "Retrieved Documents:\n",
      "- Purpose of Re-Ranking in RAG Retrieval \n",
      "The primary purpose of re-ranking in RAG retrieval is to improve the quality of the top-k \n",
      "candidates retrieved during the initial search. This is achieved by applying additional \n",
      "ranking criteria or incorporating contextual information to better align the candidates \n",
      "with the user’s query. \n",
      "1. Initial Retrieval: The system performs the initial retrieval step, finding the top-k most \n",
      "relevant responses based on vector similarity. \n",
      "2. Re-Ranking: The top-k candidates are then re-ranked using additional ranking criteria \n",
      "or contextual information, resulting in a refined list of responses. \n",
      "3. Generation: The refined list of top-k responses is fed into the language model, which \n",
      "generates the final answer based on the updated information. \n",
      "Performance Improvement in LLMs \n",
      "Re-ranking offers several performance improvements for LLM RAG retrieval systems: \n",
      "1. Enhanced Relevance: By applying additional ranking criteria, re-ranking helps ensure\n",
      "- parameters and strategies to achieve optimal retrieval performance. \n",
      "Best Practices for Implementing Re-Ranking in LLM RAG Retrieval \n",
      "To make the most of re-ranking in your LLM RAG retrieval system, follow these best \n",
      "practices: \n",
      "1. Understand Your Use Case: Different applications may require different re-\n",
      "ranking techniques. Before implementing a re-ranking strategy, carefully analyze \n",
      "your use case and determine which approach best suits your needs. \n",
      "2. Choose the Right Re-Ranking Technique: Select a re-ranking technique that \n",
      "aligns with your use case and offers the desired balance between accuracy, \n",
      "diversity, and computational efficiency. You may also consider combining \n",
      "multiple techniques for improved performance. \n",
      "3. Evaluate and Iterate: Regularly evaluate the performance of your re-ranking \n",
      "strategy using relevant metrics, such as precision, recall, or user satisfaction. \n",
      "Based on the results, iterate and refine your approach to continuously improve\n",
      "- Re-Ranking \n",
      "Now that we have a solid understanding of RAG retrieval and its benefits, let’s move on \n",
      "to the next section, where we’ll explore the role of re-ranking in further enhancing the \n",
      "performance of LLM RAG retrieval systems. \n",
      "Welcome back to our journey through the world of AI-powered systems! In this section, \n",
      "we’ll introduce re-ranking and explore its role in enhancing the performance of LLM RAG \n",
      "retrieval systems. \n",
      "Introduction to Re-Ranking \n",
      "Re-ranking is a technique used to refine the initial search results obtained from a \n",
      "retrieval system, with the goal of improving their relevance and accuracy. In the context \n",
      "of RAG retrieval, re-ranking acts as a quality control mechanism that helps our librarian \n",
      "(the LLM) fine-tune the list of potential responses before generating the final answer.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the main components of a RAG system?\"\n",
    "retrieved_docs = semantic_search(query)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Retrieved Documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(\"-\", doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f5b04935203ee",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6. Create basic QA prompt.\n",
    "7. Determine 5 evaluation queries:\n",
    "    - Determine a few questions, which answers are confirmed by you.\n",
    "8. Compare performance of RAG vs. pure LLM response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3c9d2f5328204b2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-15T21:16:58.006305Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(reference: str, generated: str) -> Dict[str, float]:\n",
    "    reference_tokens = set(reference.split())\n",
    "    generated_tokens = set(generated.split())\n",
    "\n",
    "    common_tokens = reference_tokens & generated_tokens\n",
    "\n",
    "    precision = len(common_tokens) / len(generated_tokens) if generated_tokens else 0\n",
    "    recall = len(common_tokens) / len(reference_tokens) if reference_tokens else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    return {\"Precision\": precision, \"F1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d829a53fa4d9aa12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T21:50:53.009625Z",
     "start_time": "2024-12-15T21:42:57.371779Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RAG vs Pure LLM evaluation...\n",
      "Starting evaluation at 2024-12-15 22:43:00\n",
      "================================================================================\n",
      "\n",
      "Processing question: What is re-ranking in RAG?\n",
      "\n",
      "RAG Response: Re-ranking in RAG involves using additional criteria or contextual information after initial retrieval to select the most relevant responses for language model generation.\n",
      "RAG Metrics: {'Precision': 0.30434782608695654, 'F1': 0.28571428571428575}\n",
      "RAG Processing Time: 72.78 seconds\n",
      "\n",
      "Pure LLM Response: Re-ranking in RAG involves using language models to refine or modify initial retrieval results, with the goal of improving their relevance for specific tasks. This process uses additional information from large language models (LLMs) like GPT-3 to reorder a set of documents based on contextual similarity and task-specific requirements, ultimately producing more accurate and coherent responses in conversational AI systems or document retrieval applications.\n",
      "Pure LLM Metrics: {'Precision': 0.2857142857142857, 'F1': 0.3902439024390244}\n",
      "Pure LLM Processing Time: 17.73 seconds\n",
      "\n",
      "Expected Answer: Re-ranking is a process in RAG where retrieved documents are reordered based on their relevance to the query, often using more sophisticated models than the initial retrieval.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing question: How does RAG improve response accuracy?\n",
      "\n",
      "RAG Response: RAG improves response accuracy by using re-ranking techniques that align retrieved candidates with the user's query based on additional criteria or contextual information. This refines the initial search results, ensuring higher relevance and precision in the final generated answers.\n",
      "RAG Metrics: {'Precision': 0.21621621621621623, 'F1': 0.25806451612903225}\n",
      "RAG Processing Time: 82.53 seconds\n",
      "\n",
      "Pure LLM Response: RAG improves response accuracy by quickly retrieving relevant information from large-scale datasets or precomputed representations, which the model then uses to generate more accurate responses. This process reduces reliance on extensive language models alone, thereby enhancing precision and recall in various tasks such as question answering, text summarization, etc.\n",
      "Pure LLM Metrics: {'Precision': 0.20408163265306123, 'F1': 0.2702702702702703}\n",
      "Pure LLM Processing Time: 25.01 seconds\n",
      "\n",
      "Expected Answer: RAG improves accuracy by retrieving relevant documents from a knowledge base and using them as context for generating responses, combining external knowledge with the model's capabilities.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing question: What are the main components of a RAG system?\n",
      "\n",
      "RAG Response: The primary purpose of re-ranking in RAG retrieval is to enhance candidate quality, followed by generation using language models based on refined information. Best practices include understanding your use case, choosing the right technique, and evaluating performance for iterations. Re-ranking acts as a fine-tuning mechanism between initial search results and final answer generated from LLMs.\n",
      "RAG Metrics: {'Precision': 0.18518518518518517, 'F1': 0.24691358024691354}\n",
      "RAG Processing Time: 76.66 seconds\n",
      "\n",
      "Pure LLM Response: The main components of a RAG system include an encoder (such as BERT or GPT-3), a retriever, decoder (usinG transformer architecture like T5 or UniLM) ,and integration layer that combines the outputs from both to generate answers.\n",
      "Pure LLM Metrics: {'Precision': 0.22857142857142856, 'F1': 0.25806451612903225}\n",
      "Pure LLM Processing Time: 14.40 seconds\n",
      "\n",
      "Expected Answer: The main components of RAG are: a document store, an embedding model for retrieval, a retriever for finding relevant documents, and a language model for generating responses based on the retrieved context.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing question: What is the role of embeddings in RAG?\n",
      "\n",
      "RAG Response: Embeddings represent queries, documents, or passages as high-dimensional vectors to enable similarity comparisons during retrieval. They help identify relevant information for response generation in LLM RAG systems by measuring the closeness of user input to potential answers based on vector space models.\n",
      "RAG Metrics: {'Precision': 0.21951219512195122, 'F1': 0.28125}\n",
      "RAG Processing Time: 74.83 seconds\n",
      "\n",
      "Pure LLM Response: The primary function of embeddings within Retrieval Augmented Generation (RAG) systems is to encode queries or documents into high-dimensional vector representations. These embeddings allow the model to understand semantic similarities by measuring distances between vectors, which it uses for efficient retrieval from a large corpus during text generation tasks.\n",
      "Pure LLM Metrics: {'Precision': 0.19148936170212766, 'F1': 0.2571428571428572}\n",
      "Pure LLM Processing Time: 14.75 seconds\n",
      "\n",
      "Expected Answer: Embeddings in RAG convert text into vector representations, enabling semantic search to find relevant documents by measuring similarity between query and document vectors.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing question: How does RAG handle document retrieval?\n",
      "\n",
      "RAG Response: RAG uses semantic search to retrieve relevant documents based on the similarity of their embeddings to the query.\n",
      "RAG Metrics: {'Precision': 0.3125, 'F1': 0.24390243902439027}\n",
      "RAG Processing Time: 70.38 seconds\n",
      "\n",
      "Pure LLM Response: RAG handles document retrieval by using large language models, such as BERT or GPT-3. It employs these models to encode both the query and the documents in question into high-dimensional vectors, which are then used for matching relevance between them. The retrieved document is further preprocessed (tokenization) before being inputted back into a smaller language model like T5 that generates an answer or response based on this contextual information. This process allows RAG to handle large volumes of documents efficiently while maintaining high accuracy and precision in the generated responses.\n",
      "Pure LLM Metrics: {'Precision': 0.1794871794871795, 'F1': 0.27184466019417475}\n",
      "Pure LLM Processing Time: 23.46 seconds\n",
      "\n",
      "Expected Answer: RAG handles document retrieval by converting the query into an embedding, searching for similar document embeddings in the vector database, and retrieving the most relevant documents as context.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "EVALUATION REPORT\n",
      "================================================================================\n",
      "\n",
      "Question: What is re-ranking in RAG?\n",
      "----------------------------------------\n",
      "RAG Response (72.78s):\n",
      "Re-ranking in RAG involves using additional criteria or contextual information after initial retrieval to select the most relevant responses for language model generation.\n",
      "RAG Metrics: {'Precision': 0.30434782608695654, 'F1': 0.28571428571428575}\n",
      "\n",
      "Pure LLM Response (17.73s):\n",
      "Re-ranking in RAG involves using language models to refine or modify initial retrieval results, with the goal of improving their relevance for specific tasks. This process uses additional information from large language models (LLMs) like GPT-3 to reorder a set of documents based on contextual similarity and task-specific requirements, ultimately producing more accurate and coherent responses in conversational AI systems or document retrieval applications.\n",
      "Pure LLM Metrics: {'Precision': 0.2857142857142857, 'F1': 0.3902439024390244}\n",
      "\n",
      "Expected Answer:\n",
      "Re-ranking is a process in RAG where retrieved documents are reordered based on their relevance to the query, often using more sophisticated models than the initial retrieval.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question: How does RAG improve response accuracy?\n",
      "----------------------------------------\n",
      "RAG Response (82.53s):\n",
      "RAG improves response accuracy by using re-ranking techniques that align retrieved candidates with the user's query based on additional criteria or contextual information. This refines the initial search results, ensuring higher relevance and precision in the final generated answers.\n",
      "RAG Metrics: {'Precision': 0.21621621621621623, 'F1': 0.25806451612903225}\n",
      "\n",
      "Pure LLM Response (25.01s):\n",
      "RAG improves response accuracy by quickly retrieving relevant information from large-scale datasets or precomputed representations, which the model then uses to generate more accurate responses. This process reduces reliance on extensive language models alone, thereby enhancing precision and recall in various tasks such as question answering, text summarization, etc.\n",
      "Pure LLM Metrics: {'Precision': 0.20408163265306123, 'F1': 0.2702702702702703}\n",
      "\n",
      "Expected Answer:\n",
      "RAG improves accuracy by retrieving relevant documents from a knowledge base and using them as context for generating responses, combining external knowledge with the model's capabilities.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question: What are the main components of a RAG system?\n",
      "----------------------------------------\n",
      "RAG Response (76.66s):\n",
      "The primary purpose of re-ranking in RAG retrieval is to enhance candidate quality, followed by generation using language models based on refined information. Best practices include understanding your use case, choosing the right technique, and evaluating performance for iterations. Re-ranking acts as a fine-tuning mechanism between initial search results and final answer generated from LLMs.\n",
      "RAG Metrics: {'Precision': 0.18518518518518517, 'F1': 0.24691358024691354}\n",
      "\n",
      "Pure LLM Response (14.40s):\n",
      "The main components of a RAG system include an encoder (such as BERT or GPT-3), a retriever, decoder (usinG transformer architecture like T5 or UniLM) ,and integration layer that combines the outputs from both to generate answers.\n",
      "Pure LLM Metrics: {'Precision': 0.22857142857142856, 'F1': 0.25806451612903225}\n",
      "\n",
      "Expected Answer:\n",
      "The main components of RAG are: a document store, an embedding model for retrieval, a retriever for finding relevant documents, and a language model for generating responses based on the retrieved context.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question: What is the role of embeddings in RAG?\n",
      "----------------------------------------\n",
      "RAG Response (74.83s):\n",
      "Embeddings represent queries, documents, or passages as high-dimensional vectors to enable similarity comparisons during retrieval. They help identify relevant information for response generation in LLM RAG systems by measuring the closeness of user input to potential answers based on vector space models.\n",
      "RAG Metrics: {'Precision': 0.21951219512195122, 'F1': 0.28125}\n",
      "\n",
      "Pure LLM Response (14.75s):\n",
      "The primary function of embeddings within Retrieval Augmented Generation (RAG) systems is to encode queries or documents into high-dimensional vector representations. These embeddings allow the model to understand semantic similarities by measuring distances between vectors, which it uses for efficient retrieval from a large corpus during text generation tasks.\n",
      "Pure LLM Metrics: {'Precision': 0.19148936170212766, 'F1': 0.2571428571428572}\n",
      "\n",
      "Expected Answer:\n",
      "Embeddings in RAG convert text into vector representations, enabling semantic search to find relevant documents by measuring similarity between query and document vectors.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question: How does RAG handle document retrieval?\n",
      "----------------------------------------\n",
      "RAG Response (70.38s):\n",
      "RAG uses semantic search to retrieve relevant documents based on the similarity of their embeddings to the query.\n",
      "RAG Metrics: {'Precision': 0.3125, 'F1': 0.24390243902439027}\n",
      "\n",
      "Pure LLM Response (23.46s):\n",
      "RAG handles document retrieval by using large language models, such as BERT or GPT-3. It employs these models to encode both the query and the documents in question into high-dimensional vectors, which are then used for matching relevance between them. The retrieved document is further preprocessed (tokenization) before being inputted back into a smaller language model like T5 that generates an answer or response based on this contextual information. This process allows RAG to handle large volumes of documents efficiently while maintaining high accuracy and precision in the generated responses.\n",
      "Pure LLM Metrics: {'Precision': 0.1794871794871795, 'F1': 0.27184466019417475}\n",
      "\n",
      "Expected Answer:\n",
      "RAG handles document retrieval by converting the query into an embedding, searching for similar document embeddings in the vector database, and retrieving the most relevant documents as context.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "SUMMARY STATISTICS\n",
      "----------------------------------------\n",
      "Total questions evaluated: 5\n",
      "Average RAG processing time: 75.43 seconds\n",
      "Average Pure LLM processing time: 19.07 seconds\n",
      "Total evaluation time: 472.52 seconds\n",
      "Question: What is re-ranking in RAG?\n",
      "RAG Metrics: {'Precision': 0.30434782608695654, 'F1': 0.28571428571428575}\n",
      "Pure LLM Metrics: {'Precision': 0.2857142857142857, 'F1': 0.3902439024390244}\n",
      "==================================================\n",
      "Question: How does RAG improve response accuracy?\n",
      "RAG Metrics: {'Precision': 0.21621621621621623, 'F1': 0.25806451612903225}\n",
      "Pure LLM Metrics: {'Precision': 0.20408163265306123, 'F1': 0.2702702702702703}\n",
      "==================================================\n",
      "Question: What are the main components of a RAG system?\n",
      "RAG Metrics: {'Precision': 0.18518518518518517, 'F1': 0.24691358024691354}\n",
      "Pure LLM Metrics: {'Precision': 0.22857142857142856, 'F1': 0.25806451612903225}\n",
      "==================================================\n",
      "Question: What is the role of embeddings in RAG?\n",
      "RAG Metrics: {'Precision': 0.21951219512195122, 'F1': 0.28125}\n",
      "Pure LLM Metrics: {'Precision': 0.19148936170212766, 'F1': 0.2571428571428572}\n",
      "==================================================\n",
      "Question: How does RAG handle document retrieval?\n",
      "RAG Metrics: {'Precision': 0.3125, 'F1': 0.24390243902439027}\n",
      "Pure LLM Metrics: {'Precision': 0.1794871794871795, 'F1': 0.27184466019417475}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "model = ChatOllama(model=\"phi3:3.8b\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "client = QdrantClient(url=\"http://127.0.0.1:6333\")\n",
    "collection_name = \"cv_collection\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful AI assistant specialized in answering questions about RAG (Retrieval Augmented Generation).\n",
    "Answer the question based ONLY on the provided context.\n",
    "If the answer cannot be found in the context, say \"I cannot find the answer in the available context.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear and concise answer:\n",
    "\"\"\")\n",
    "\n",
    "pure_llm_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful AI assistant specialized in answering questions about RAG (Retrieval Augmented Generation).\n",
    "Answer the following question as accurately as possible based on your knowledge:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear and concise answer:\n",
    "\"\"\")\n",
    "\n",
    "def get_rag_response(question: str) -> Dict[str, any]:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    context = \"\\n\\n\".join(semantic_search(question))\n",
    "    \n",
    "    chain = qa_prompt | model | StrOutputParser()\n",
    "    response = chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"context_used\": context,\n",
    "        \"processing_time\": processing_time\n",
    "    }\n",
    "\n",
    "def get_pure_llm_response(question: str) -> Dict[str, any]:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    chain = pure_llm_prompt | model | StrOutputParser()\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"processing_time\": processing_time\n",
    "    }\n",
    "\n",
    "evaluation_queries = [\n",
    "    {\n",
    "        \"question\": \"What is re-ranking in RAG?\",\n",
    "        \"expected_answer\": \"Re-ranking is a process in RAG where retrieved documents are reordered based on their relevance to the query, often using more sophisticated models than the initial retrieval.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does RAG improve response accuracy?\",\n",
    "        \"expected_answer\": \"RAG improves accuracy by retrieving relevant documents from a knowledge base and using them as context for generating responses, combining external knowledge with the model's capabilities.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the main components of a RAG system?\",\n",
    "        \"expected_answer\": \"The main components of RAG are: a document store, an embedding model for retrieval, a retriever for finding relevant documents, and a language model for generating responses based on the retrieved context.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the role of embeddings in RAG?\",\n",
    "        \"expected_answer\": \"Embeddings in RAG convert text into vector representations, enabling semantic search to find relevant documents by measuring similarity between query and document vectors.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does RAG handle document retrieval?\",\n",
    "        \"expected_answer\": \"RAG handles document retrieval by converting the query into an embedding, searching for similar document embeddings in the vector database, and retrieving the most relevant documents as context.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def compare_responses(queries: List[Dict[str, str]]) -> Dict[str, Dict]:\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"Starting evaluation at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for query in queries:\n",
    "        question = query[\"question\"]\n",
    "        expected = query[\"expected_answer\"]\n",
    "        \n",
    "        print(f\"\\nProcessing question: {question}\")\n",
    "        \n",
    "        rag_result = get_rag_response(question)\n",
    "        pure_result = get_pure_llm_response(question)\n",
    "        \n",
    "        rag_metrics = calculate_metrics(expected, rag_result[\"response\"])\n",
    "        pure_metrics = calculate_metrics(expected, pure_result[\"response\"])\n",
    "        \n",
    "        results[question] = {\n",
    "            \"rag\": {\n",
    "                \"response\": rag_result[\"response\"],\n",
    "                \"processing_time\": rag_result[\"processing_time\"],\n",
    "                \"metrics\": rag_metrics\n",
    "            },\n",
    "            \"pure_llm\": {\n",
    "                \"response\": pure_result[\"response\"],\n",
    "                \"processing_time\": pure_result[\"processing_time\"],\n",
    "                \"metrics\": pure_metrics\n",
    "            },\n",
    "            \"expected\": expected\n",
    "        }\n",
    "        \n",
    "        print(\"\\nRAG Response:\", rag_result[\"response\"])\n",
    "        print(f\"RAG Metrics: {rag_metrics}\")\n",
    "        print(f\"RAG Processing Time: {rag_result['processing_time']:.2f} seconds\")\n",
    "        print(\"\\nPure LLM Response:\", pure_result[\"response\"])\n",
    "        print(f\"Pure LLM Metrics: {pure_metrics}\")\n",
    "        print(f\"Pure LLM Processing Time: {pure_result['processing_time']:.2f} seconds\")\n",
    "        print(\"\\nExpected Answer:\", expected)\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_evaluation_report(results: Dict[str, Dict]) -> None:\n",
    "    print(\"\\nEVALUATION REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    total_rag_time = 0\n",
    "    total_pure_time = 0\n",
    "    \n",
    "    for question, data in results.items():\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        rag_response = data[\"rag\"][\"response\"]\n",
    "        rag_metrics = data[\"rag\"][\"metrics\"]\n",
    "        pure_response = data[\"pure_llm\"][\"response\"]\n",
    "        pure_metrics = data[\"pure_llm\"][\"metrics\"]\n",
    "        expected = data[\"expected\"]\n",
    "        \n",
    "        print(f\"RAG Response ({data['rag']['processing_time']:.2f}s):\")\n",
    "        print(rag_response)\n",
    "        print(f\"RAG Metrics: {rag_metrics}\")\n",
    "        \n",
    "        print(f\"\\nPure LLM Response ({data['pure_llm']['processing_time']:.2f}s):\")\n",
    "        print(pure_response)\n",
    "        print(f\"Pure LLM Metrics: {pure_metrics}\")\n",
    "        \n",
    "        print(f\"\\nExpected Answer:\")\n",
    "        print(expected)\n",
    "        \n",
    "        total_rag_time += data[\"rag\"][\"processing_time\"]\n",
    "        total_pure_time += data[\"pure_llm\"][\"processing_time\"]\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "    \n",
    "    num_questions = len(results)\n",
    "    avg_rag_time = total_rag_time / num_questions\n",
    "    avg_pure_time = total_pure_time / num_questions\n",
    "    \n",
    "    print(\"\\nSUMMARY STATISTICS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total questions evaluated: {num_questions}\")\n",
    "    print(f\"Average RAG processing time: {avg_rag_time:.2f} seconds\")\n",
    "    print(f\"Average Pure LLM processing time: {avg_pure_time:.2f} seconds\")\n",
    "    print(f\"Total evaluation time: {(total_rag_time + total_pure_time):.2f} seconds\")\n",
    "\n",
    "def run_evaluation():\n",
    "    print(\"Starting RAG vs Pure LLM evaluation...\")\n",
    "    results = compare_responses(evaluation_queries)\n",
    "    generate_evaluation_report(results)\n",
    "    return results\n",
    "\n",
    "results = run_evaluation()\n",
    "\n",
    "for question, data in results.items():\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"RAG Metrics: {data['rag']['metrics']}\")\n",
    "    print(f\"Pure LLM Metrics: {data['pure_llm']['metrics']}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f901badc09de218c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The evaluation highlights clear differences between RAG pipeline and the pure LLM approach. While RAG demonstrates the ability to generate responses grounded in external knowledge bases, it suffers from longer processing times, averaging 75.43 seconds per query compared to 19.07 seconds for the pure LLM. This difference is primarily due to the semantic search and embedding generation steps in the RAG pipeline.\n",
    "\n",
    "RAG responses are more contextually accurate and closely aligned with the retrieved documents, reducing the risk of hallucinations. However, they often lack the depth and detail seen in pure LLM responses. In contrast, the pure LLM provides more detailed and creative answers, but these answers can include fabricated or inaccurate information as they rely solely on the model's pre-trained knowledge.\n",
    "\n",
    "In terms of F1 scores, the pure LLM generally outperformed RAG, indicating better overall alignment with the expected answers. However, RAG showed potential in scenarios requiring high contextual reliability and external validation, such as document-grounded queries.\n",
    "\n",
    "In conclusion, RAG is better suited for tasks requiring strict adherence to external data, while the pure LLM excels in generating faster and more comprehensive responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2659ceaef7c59c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Questions (2 points):\n",
    "\n",
    "**1. How does RAG improve the quality and reliability of LLM responses compared to pure LLM generation?**\n",
    "\n",
    "RAG improves reliability by grounding responses in retrieved documents, reducing hallucinations common in pure LLMs. This ensures answers are more accurate and contextually relevant. However, RAG is slower, while pure LLMs generate quicker but less reliable responses that may include fabricated information.\n",
    "\n",
    "**2. What are the key factors affecting RAG performance (chunk size, embedding quality, prompt design)?**\n",
    "\n",
    "Chunk size affects retrieval granularity; smaller chunks improve precision, while larger ones provide more context. Embedding quality ensures better semantic matching. Prompt design helps guide the model to focus strictly on retrieved context, avoiding irrelevant details.\n",
    "\n",
    "**3. How does the choice of vector database and embedding model impact system performance?**\n",
    "\n",
    "A fast, scalable vector database ensures quick and accurate retrieval. Embedding models like all-MiniLM-L6-v2 balance efficiency and precision, while larger models improve retrieval accuracy but increase latency and computational costs.\n",
    "\n",
    "**4. What are the main challenges in implementing a production-ready RAG system?**\n",
    "\n",
    "Key challenges include high latency, scalability for large datasets, ensuring retrieval accuracy, and integrating retrieval with generation reliably. Handling complex queries requiring cross-document synthesis is also a significant hurdle.\n",
    "\n",
    "**5. How can the system be improved to handle complex queries requiring multiple document lookups?**\n",
    "\n",
    "Hierarchical retrieval can narrow search results, while query expansion improves recall. Metadata linking helps retrieve related chunks, and dynamic context aggregation combines information effectively. Feedback mechanisms can refine retrieval accuracy over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f91072f247fef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hints\n",
    "\n",
    "1. Careful chunk size selection is crucial for relevant context retrieval\n",
    "2. Consider implementing re-ranking of retrieved documents\n",
    "3. Prompt engineering significantly impacts answer quality\n",
    "4. Caching can greatly improve system performance during development\n",
    "5. Consider using metadata filtering to improve retrieval precision\n",
    "6. The choice of embedding model affects both accuracy and speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0d89f47c4b2f0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
