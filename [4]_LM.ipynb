{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Language modelling\n",
    "\n",
    "The exercise shows how a language model may be used to solve word-prediction tasks and to generate text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f286d77ed6a9526c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tasks\n",
    "\n",
    "Objectives (8 points):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f02a01f286d3ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Imports**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efa68ad8470f6a81"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T20:26:08.652323100Z",
     "start_time": "2024-11-08T20:25:49.110350200Z"
    }
   },
   "id": "1c51387e32ece0f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Read the documentation of [Language modelling in the Transformers](https://huggingface.co/transformers/task_summary.html#language-modeling) library."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4737f62ba1ab6d97"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Acer\\Desktop\\nlp\\venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cd2edd41f0e4a9aa4c21b3fe52af13b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\Desktop\\nlp\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Acer\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Acer\\Desktop\\nlp\\venv\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d03ccae8b404b848aa1c6ef75301b47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c7e193105044093921a69c46a057fb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e91c19cd25fa4dcd9e2555108bab9d19"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91587a4df9784e08b325468af8452b02"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'generated_text': \"Hello, It's me and I am testing this generator on my computer. For about a day. I need a big hard drive with plenty of space for everything. I just need it to generate some code. To do this I can run this:\"}]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(task=\"text-generation\")\n",
    "prompt = \"Hello, It's me and I am testing this generator\"\n",
    "generator(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T20:27:54.545868100Z",
     "start_time": "2024-11-08T20:26:31.918766200Z"
    }
   },
   "id": "7760d2fc99942b5a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "235042aba037462e90309ade9794b49f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\Desktop\\nlp\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Acer\\.cache\\huggingface\\hub\\models--distilbert--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4164dd9e2e5e40a7bd7de50036666ed6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6715bbcd65e4b27a51093a8f92b0c91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f366e1573de0464cac8ede4b81bdc89b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b5cfdccc9d34d0f8d93be54dad487c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8906b8fe68a24feaaa9f5abb70ccd1d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'score': 0.2236,\n  'token': 1761,\n  'token_str': ' platform',\n  'sequence': 'Hugging Face is a community-based open-source platform for machine learning.'}]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hugging Face is a community-based open-source <mask> for machine learning.\"\n",
    "fill_mask = pipeline(task=\"fill-mask\")\n",
    "preds = fill_mask(text, top_k=1)\n",
    "preds = [\n",
    "    {\n",
    "        \"score\": round(pred[\"score\"], 4),\n",
    "        \"token\": pred[\"token\"],\n",
    "        \"token_str\": pred[\"token_str\"],\n",
    "        \"sequence\": pred[\"sequence\"],\n",
    "    }\n",
    "    for pred in preds\n",
    "]\n",
    "\n",
    "preds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T20:33:16.845869700Z",
     "start_time": "2024-11-08T20:32:44.596567600Z"
    }
   },
   "id": "8823440c921fc95f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Download three [Polish models](https://huggingface.co/models?filter=pl) from the Huggingface repository. These should be regular language models, which were not fine-tuned. E.g. `HerBERT` and `papuGaPT2` are good examples. You can also try using Bielik for that, but make sure you are using the model via Transformers API, not GUI."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0e9d41ed4a19cd2"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82ee9d5fd9a7432db7835028b1e1da7c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\Desktop\\nlp\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Acer\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d74b9bc5dce4843b028e50607c72999"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5013886e877a4cc7b66013d0a616b5e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "521a71f4e408411f98620270670270e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99db43f733fa48fe9fcf80be362abc7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2abceb0a89bb4849bf4c4f13b52d5808"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\Desktop\\nlp\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Acer\\.cache\\huggingface\\hub\\models--distilbert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "947a9de7539645a7b9aafec16ede627c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the weights of TFDistilBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f536132243f402788f4820bb2714df6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28f897b87e3b46ab9d0c2e59c0b216d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc4d55cbe56d4dd5abb47b0874cdb0ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e71d156f09f4ce9b4bda91116513d97"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\Desktop\\nlp\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Acer\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/1.12G [00:02<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "068fe5a21498430db08104f2522bf80e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFXLMRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFXLMRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7602ad58c06427ebc0a8e5ab62bc5b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ef1c369f50648a69279df5a85919c9a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5ac53c8e481474bbd330365d255d401"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert = pipeline('fill-mask', model='bert-base-multilingual-cased')\n",
    "distilbert = pipeline('fill-mask', model='distilbert-base-multilingual-cased')\n",
    "roberta = pipeline(\"fill-mask\", model='xlm-roberta-base')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T20:40:48.450945300Z",
     "start_time": "2024-11-08T20:36:59.885845600Z"
    }
   },
   "id": "3bbccf074cee0d1e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "models = [\n",
    "    'bert-base-multilingual-cased',\n",
    "    'distilbert-base-multilingual-cased',\n",
    "    'xlm-roberta-base'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T20:48:28.036305200Z",
     "start_time": "2024-11-08T20:48:27.281782Z"
    }
   },
   "id": "f9acf6736849935b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**BERT Base Multilingual Cased** is a transformer model pretrained on text from 104 languages, primarily Wikipedia. It excels in tasks like text classification and question answering, using masked language modeling and next sentence prediction. Its case-sensitive nature allows it to differentiate between capitalized and lowercase words.\n",
    "\n",
    "**DistilBERT Base Multilingual Cased** is a lighter and faster version of BERT, retaining 97% of its language understanding capabilities while being 60% faster and more memory-efficient. It is also pretrained on a multilingual corpus and is effective for various NLP tasks, preserving case distinctions.\n",
    "\n",
    "**XLM-RoBERTa Base** is built on the RoBERTa architecture and trained on 100 languages. It uses advanced training techniques to enhance performance, especially for low-resource languages. This model is particularly strong in cross-lingual tasks and translation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bbf37e4b7124956"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Devise a method to test if the langage model understands Polish cases. E.g. testing for *nominal case* could be expressed as \"Warszawa to największe `[MASK]`\", and the masked word should be in nominative case. Create sentences for each case."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a05e9b6572dab9d2"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def fill_mask_for_model(text, model_name: str, k = 5):\n",
    "    results = []\n",
    "    \n",
    "    if model_name == 'xlm-roberta-base':\n",
    "        text = text.replace('[MASK]', '<mask>')\n",
    "    \n",
    "    model = pipeline('fill-mask', model=model_name)\n",
    "    preds = model(text, top_k=k)\n",
    "\n",
    "    for pred in preds:\n",
    "        result = {\n",
    "            \"model\": model_name,\n",
    "            \"score\": round(pred[\"score\"], 2),\n",
    "            \"token\": pred[\"token_str\"],\n",
    "            \"sequence\": pred[\"sequence\"]\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T20:55:02.971227200Z",
     "start_time": "2024-11-08T20:55:02.846351700Z"
    }
   },
   "id": "9241893e06e5a1cc"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def fill_mask_for_all_models_and_return_pandas(text, k = 5):\n",
    "    result_list = []\n",
    "\n",
    "    for model in models:\n",
    "        result_list += fill_mask_for_model(text, model, k)\n",
    "    \n",
    "    return pd.DataFrame(result_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T20:56:32.360981400Z",
     "start_time": "2024-11-08T20:56:31.834020400Z"
    }
   },
   "id": "744b1db1c6aa1b51"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "df_bert_base = pd.DataFrame(fill_mask_for_model('Warszawa to największe [MASK]', models[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T20:58:20.261075300Z",
     "start_time": "2024-11-08T20:57:44.430057300Z"
    }
   },
   "id": "917dd7253c1f8599"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "                          model  score   token                       sequence\n0  bert-base-multilingual-cased   0.62       .        Warszawa to największe.\n1  bert-base-multilingual-cased   0.16  miasto  Warszawa to największe miasto\n2  bert-base-multilingual-cased   0.03  Miasto  Warszawa to największe Miasto\n3  bert-base-multilingual-cased   0.02  miasta  Warszawa to największe miasta\n4  bert-base-multilingual-cased   0.01       :       Warszawa to największe :",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score</th>\n      <th>token</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.62</td>\n      <td>.</td>\n      <td>Warszawa to największe.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.16</td>\n      <td>miasto</td>\n      <td>Warszawa to największe miasto</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>Miasto</td>\n      <td>Warszawa to największe Miasto</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>miasta</td>\n      <td>Warszawa to największe miasta</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.01</td>\n      <td>:</td>\n      <td>Warszawa to największe :</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bert_base"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T20:58:24.867585400Z",
     "start_time": "2024-11-08T20:58:24.704848500Z"
    }
   },
   "id": "d48de2f8095c2eef"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the weights of TFDistilBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "df_distilbert = pd.DataFrame(fill_mask_for_model('Warszawa to największe [MASK]', models[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T20:59:00.526434200Z",
     "start_time": "2024-11-08T20:58:42.060649100Z"
    }
   },
   "id": "59f444886466a518"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "                                model  score   token  \\\n0  distilbert-base-multilingual-cased   0.56  miasto   \n1  distilbert-base-multilingual-cased   0.10  miasta   \n2  distilbert-base-multilingual-cased   0.02  Miasto   \n3  distilbert-base-multilingual-cased   0.01       .   \n4  distilbert-base-multilingual-cased   0.01   ##sza   \n\n                        sequence  \n0  Warszawa to największe miasto  \n1  Warszawa to największe miasta  \n2  Warszawa to największe Miasto  \n3        Warszawa to największe.  \n4      Warszawa to największesza  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score</th>\n      <th>token</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.56</td>\n      <td>miasto</td>\n      <td>Warszawa to największe miasto</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.10</td>\n      <td>miasta</td>\n      <td>Warszawa to największe miasta</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>Miasto</td>\n      <td>Warszawa to największe Miasto</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.01</td>\n      <td>.</td>\n      <td>Warszawa to największe.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.01</td>\n      <td>##sza</td>\n      <td>Warszawa to największesza</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_distilbert"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T20:59:05.075697500Z",
     "start_time": "2024-11-08T20:59:04.252108600Z"
    }
   },
   "id": "491eb5c5eb6b43fe"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFXLMRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFXLMRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "df_roberta = pd.DataFrame(fill_mask_for_model('Warszawa to największe [MASK]', models[2]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T21:01:05.603501100Z",
     "start_time": "2024-11-08T21:00:04.000341600Z"
    }
   },
   "id": "b0c7d9b965bf4e6a"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "              model  score   token                       sequence\n0  xlm-roberta-base   0.30     ...      Warszawa to największe...\n1  xlm-roberta-base   0.16  miasto  Warszawa to największe miasto\n2  xlm-roberta-base   0.07       !        Warszawa to największe!\n3  xlm-roberta-base   0.05     ...     Warszawa to największe ...\n4  xlm-roberta-base   0.05    city    Warszawa to największe city",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score</th>\n      <th>token</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>xlm-roberta-base</td>\n      <td>0.30</td>\n      <td>...</td>\n      <td>Warszawa to największe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>xlm-roberta-base</td>\n      <td>0.16</td>\n      <td>miasto</td>\n      <td>Warszawa to największe miasto</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>xlm-roberta-base</td>\n      <td>0.07</td>\n      <td>!</td>\n      <td>Warszawa to największe!</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>xlm-roberta-base</td>\n      <td>0.05</td>\n      <td>...</td>\n      <td>Warszawa to największe ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>xlm-roberta-base</td>\n      <td>0.05</td>\n      <td>city</td>\n      <td>Warszawa to największe city</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_roberta"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T21:01:08.304745800Z",
     "start_time": "2024-11-08T21:01:07.189097500Z"
    }
   },
   "id": "142c0186619380a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Devise a method to test long-range relationships such as gender. E.e. you can use two verbs with masculine and feminine gender, where one of the verbs is masked. Both verbs should have the same gender, assuming the subject is the same. Define at least 3 such sentences."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2da10a7b8dff42f"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the weights of TFDistilBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFXLMRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 model  score       token  \\\n0         bert-base-multilingual-cased   0.35         sam   \n1         bert-base-multilingual-cased   0.07        może   \n2         bert-base-multilingual-cased   0.04          to   \n3         bert-base-multilingual-cased   0.04         tak   \n4         bert-base-multilingual-cased   0.04          by   \n5   distilbert-base-multilingual-cased   0.13        miał   \n6   distilbert-base-multilingual-cased   0.05         ten   \n7   distilbert-base-multilingual-cased   0.04           魂   \n8   distilbert-base-multilingual-cased   0.03         sam   \n9   distilbert-base-multilingual-cased   0.03       tylko   \n10                    xlm-roberta-base   0.33        mógł   \n11                    xlm-roberta-base   0.18      musiał   \n12                    xlm-roberta-base   0.17      chciał   \n13                    xlm-roberta-base   0.07  postanowił   \n14                    xlm-roberta-base   0.05        chce   \n\n                                             sequence  \n0       Ja tak ciężko pracowałam, a on sam to zrobić.  \n1      Ja tak ciężko pracowałam, a on może to zrobić.  \n2        Ja tak ciężko pracowałam, a on to to zrobić.  \n3       Ja tak ciężko pracowałam, a on tak to zrobić.  \n4        Ja tak ciężko pracowałam, a on by to zrobić.  \n5      Ja tak ciężko pracowałam, a on miał to zrobić.  \n6       Ja tak ciężko pracowałam, a on ten to zrobić.  \n7         Ja tak ciężko pracowałam, a on 魂 to zrobić.  \n8       Ja tak ciężko pracowałam, a on sam to zrobić.  \n9     Ja tak ciężko pracowałam, a on tylko to zrobić.  \n10     Ja tak ciężko pracowałam, a on mógł to zrobić.  \n11   Ja tak ciężko pracowałam, a on musiał to zrobić.  \n12   Ja tak ciężko pracowałam, a on chciał to zrobić.  \n13  Ja tak ciężko pracowałam, a on postanowił to z...  \n14     Ja tak ciężko pracowałam, a on chce to zrobić.  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score</th>\n      <th>token</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.35</td>\n      <td>sam</td>\n      <td>Ja tak ciężko pracowałam, a on sam to zrobić.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.07</td>\n      <td>może</td>\n      <td>Ja tak ciężko pracowałam, a on może to zrobić.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.04</td>\n      <td>to</td>\n      <td>Ja tak ciężko pracowałam, a on to to zrobić.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.04</td>\n      <td>tak</td>\n      <td>Ja tak ciężko pracowałam, a on tak to zrobić.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.04</td>\n      <td>by</td>\n      <td>Ja tak ciężko pracowałam, a on by to zrobić.</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.13</td>\n      <td>miał</td>\n      <td>Ja tak ciężko pracowałam, a on miał to zrobić.</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.05</td>\n      <td>ten</td>\n      <td>Ja tak ciężko pracowałam, a on ten to zrobić.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.04</td>\n      <td>魂</td>\n      <td>Ja tak ciężko pracowałam, a on 魂 to zrobić.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>sam</td>\n      <td>Ja tak ciężko pracowałam, a on sam to zrobić.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>tylko</td>\n      <td>Ja tak ciężko pracowałam, a on tylko to zrobić.</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>xlm-roberta-base</td>\n      <td>0.33</td>\n      <td>mógł</td>\n      <td>Ja tak ciężko pracowałam, a on mógł to zrobić.</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>xlm-roberta-base</td>\n      <td>0.18</td>\n      <td>musiał</td>\n      <td>Ja tak ciężko pracowałam, a on musiał to zrobić.</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>xlm-roberta-base</td>\n      <td>0.17</td>\n      <td>chciał</td>\n      <td>Ja tak ciężko pracowałam, a on chciał to zrobić.</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>xlm-roberta-base</td>\n      <td>0.07</td>\n      <td>postanowił</td>\n      <td>Ja tak ciężko pracowałam, a on postanowił to z...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>xlm-roberta-base</td>\n      <td>0.05</td>\n      <td>chce</td>\n      <td>Ja tak ciężko pracowałam, a on chce to zrobić.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_for_all_models_and_return_pandas('Ja tak ciężko pracowałam, a on [MASK] to zrobić.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T21:04:48.955241900Z",
     "start_time": "2024-11-08T21:03:31.699663200Z"
    }
   },
   "id": "fe2ccac35b02b32c"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the weights of TFDistilBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFXLMRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 model  score      token  \\\n0         bert-base-multilingual-cased   0.31      ##dzi   \n1         bert-base-multilingual-cased   0.12     ##dzie   \n2         bert-base-multilingual-cased   0.05       ##ją   \n3         bert-base-multilingual-cased   0.03        ##ł   \n4         bert-base-multilingual-cased   0.03       ##dz   \n5   distilbert-base-multilingual-cased   0.07       było   \n6   distilbert-base-multilingual-cased   0.04      udało   \n7   distilbert-base-multilingual-cased   0.03       miał   \n8   distilbert-base-multilingual-cased   0.03        nie   \n9   distilbert-base-multilingual-cased   0.02  prowadził   \n10                    xlm-roberta-base   0.59     pomoże   \n11                    xlm-roberta-base   0.13     pomaga   \n12                    xlm-roberta-base   0.02   dziękuję   \n13                    xlm-roberta-base   0.02      pomóc   \n14                    xlm-roberta-base   0.02  wystarczy   \n\n                                             sequence  \n0   On nie potrafi tego używać i miał nadzieję że ...  \n1   On nie potrafi tego używać i miał nadzieję że ...  \n2   On nie potrafi tego używać i miał nadzieję że ...  \n3   On nie potrafi tego używać i miał nadzieję że ...  \n4   On nie potrafi tego używać i miał nadzieję że ...  \n5   On nie potrafi tego używać i miał nadzieję że ...  \n6   On nie potrafi tego używać i miał nadzieję że ...  \n7   On nie potrafi tego używać i miał nadzieję że ...  \n8   On nie potrafi tego używać i miał nadzieję że ...  \n9   On nie potrafi tego używać i miał nadzieję że ...  \n10  On nie potrafi tego używać i miał nadzieję że ...  \n11  On nie potrafi tego używać i miał nadzieję że ...  \n12  On nie potrafi tego używać i miał nadzieję że ...  \n13  On nie potrafi tego używać i miał nadzieję że ...  \n14  On nie potrafi tego używać i miał nadzieję że ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score</th>\n      <th>token</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.31</td>\n      <td>##dzi</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.12</td>\n      <td>##dzie</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.05</td>\n      <td>##ją</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>##ł</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>##dz</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.07</td>\n      <td>było</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.04</td>\n      <td>udało</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>miał</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>nie</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>prowadził</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>xlm-roberta-base</td>\n      <td>0.59</td>\n      <td>pomoże</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>xlm-roberta-base</td>\n      <td>0.13</td>\n      <td>pomaga</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>xlm-roberta-base</td>\n      <td>0.02</td>\n      <td>dziękuję</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>xlm-roberta-base</td>\n      <td>0.02</td>\n      <td>pomóc</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>xlm-roberta-base</td>\n      <td>0.02</td>\n      <td>wystarczy</td>\n      <td>On nie potrafi tego używać i miał nadzieję że ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_for_all_models_and_return_pandas('On nie potrafi tego używać i miał nadzieję że ja jemu [MASK].')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T21:08:05.007751200Z",
     "start_time": "2024-11-08T21:06:17.842680500Z"
    }
   },
   "id": "dbd2ee1049f54c0a"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the weights of TFDistilBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFXLMRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 model  score        token  \\\n0         bert-base-multilingual-cased   0.05          nie   \n1         bert-base-multilingual-cased   0.03        ##czy   \n2         bert-base-multilingual-cased   0.02          ##r   \n3         bert-base-multilingual-cased   0.02        ##rze   \n4         bert-base-multilingual-cased   0.01          sam   \n5   distilbert-base-multilingual-cased   0.13   stanowisko   \n6   distilbert-base-multilingual-cased   0.06        ##rze   \n7   distilbert-base-multilingual-cased   0.02         czas   \n8   distilbert-base-multilingual-cased   0.02        tylko   \n9   distilbert-base-multilingual-cased   0.02          nie   \n10                    xlm-roberta-base   0.80   zdecydował   \n11                    xlm-roberta-base   0.06       zgadza   \n12                    xlm-roberta-base   0.03       wybrał   \n13                    xlm-roberta-base   0.02       nadaje   \n14                    xlm-roberta-base   0.01  przygotował   \n\n                                             sequence  \n0       Chciałam iść na zakupy, ale on się na to nie.  \n1        Chciałam iść na zakupy, ale on się na toczy.  \n2          Chciałam iść na zakupy, ale on się na tor.  \n3        Chciałam iść na zakupy, ale on się na torze.  \n4       Chciałam iść na zakupy, ale on się na to sam.  \n5   Chciałam iść na zakupy, ale on się na to stano...  \n6        Chciałam iść na zakupy, ale on się na torze.  \n7      Chciałam iść na zakupy, ale on się na to czas.  \n8     Chciałam iść na zakupy, ale on się na to tylko.  \n9       Chciałam iść na zakupy, ale on się na to nie.  \n10  Chciałam iść na zakupy, ale on się na to zdecy...  \n11  Chciałam iść na zakupy, ale on się na to zgadza .  \n12  Chciałam iść na zakupy, ale on się na to wybrał .  \n13  Chciałam iść na zakupy, ale on się na to nadaje .  \n14  Chciałam iść na zakupy, ale on się na to przyg...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score</th>\n      <th>token</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.05</td>\n      <td>nie</td>\n      <td>Chciałam iść na zakupy, ale on się na to nie.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>##czy</td>\n      <td>Chciałam iść na zakupy, ale on się na toczy.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>##r</td>\n      <td>Chciałam iść na zakupy, ale on się na tor.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>##rze</td>\n      <td>Chciałam iść na zakupy, ale on się na torze.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.01</td>\n      <td>sam</td>\n      <td>Chciałam iść na zakupy, ale on się na to sam.</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.13</td>\n      <td>stanowisko</td>\n      <td>Chciałam iść na zakupy, ale on się na to stano...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.06</td>\n      <td>##rze</td>\n      <td>Chciałam iść na zakupy, ale on się na torze.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>czas</td>\n      <td>Chciałam iść na zakupy, ale on się na to czas.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>tylko</td>\n      <td>Chciałam iść na zakupy, ale on się na to tylko.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>nie</td>\n      <td>Chciałam iść na zakupy, ale on się na to nie.</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>xlm-roberta-base</td>\n      <td>0.80</td>\n      <td>zdecydował</td>\n      <td>Chciałam iść na zakupy, ale on się na to zdecy...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>xlm-roberta-base</td>\n      <td>0.06</td>\n      <td>zgadza</td>\n      <td>Chciałam iść na zakupy, ale on się na to zgadza .</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>xlm-roberta-base</td>\n      <td>0.03</td>\n      <td>wybrał</td>\n      <td>Chciałam iść na zakupy, ale on się na to wybrał .</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>xlm-roberta-base</td>\n      <td>0.02</td>\n      <td>nadaje</td>\n      <td>Chciałam iść na zakupy, ale on się na to nadaje .</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>xlm-roberta-base</td>\n      <td>0.01</td>\n      <td>przygotował</td>\n      <td>Chciałam iść na zakupy, ale on się na to przyg...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_for_all_models_and_return_pandas('Chciałam iść na zakupy, ale on się na to [MASK].')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T21:13:51.444792600Z",
     "start_time": "2024-11-08T21:12:03.124401400Z"
    }
   },
   "id": "d1415ec18db4e886"
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Check if the model captures real-world knolwedge. For instance a sentence \"`[MASK]` wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\" checks if the model \"knows\" the description of water. Define at least 3 such sentences."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ca38ee366703122"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the weights of TFDistilBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFXLMRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 model  score        token  \\\n0         bert-base-multilingual-cased   0.04         Jego   \n1         bert-base-multilingual-cased   0.03           Za   \n2         bert-base-multilingual-cased   0.03           Po   \n3         bert-base-multilingual-cased   0.02           Na   \n4         bert-base-multilingual-cased   0.02            W   \n5   distilbert-base-multilingual-cased   0.09           Na   \n6   distilbert-base-multilingual-cased   0.05           We   \n7   distilbert-base-multilingual-cased   0.03           Od   \n8   distilbert-base-multilingual-cased   0.03            W   \n9   distilbert-base-multilingual-cased   0.03           we   \n10                    xlm-roberta-base   0.11    Najlepiej   \n11                    xlm-roberta-base   0.10            -   \n12                    xlm-roberta-base   0.04  Najczęściej   \n13                    xlm-roberta-base   0.04            –   \n14                    xlm-roberta-base   0.03    Następnie   \n\n                                             sequence  \n0   Jego wrze w temperaturze 100 stopni, a zamarza...  \n1   Za wrze w temperaturze 100 stopni, a zamarza w...  \n2   Po wrze w temperaturze 100 stopni, a zamarza w...  \n3   Na wrze w temperaturze 100 stopni, a zamarza w...  \n4   W wrze w temperaturze 100 stopni, a zamarza w ...  \n5   Na wrze w temperaturze 100 stopni, a zamarza w...  \n6   We wrze w temperaturze 100 stopni, a zamarza w...  \n7   Od wrze w temperaturze 100 stopni, a zamarza w...  \n8   W wrze w temperaturze 100 stopni, a zamarza w ...  \n9   we wrze w temperaturze 100 stopni, a zamarza w...  \n10  Najlepiej wrze w temperaturze 100 stopni, a za...  \n11  - wrze w temperaturze 100 stopni, a zamarza w ...  \n12  Najczęściej wrze w temperaturze 100 stopni, a ...  \n13  – wrze w temperaturze 100 stopni, a zamarza w ...  \n14  Następnie wrze w temperaturze 100 stopni, a za...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score</th>\n      <th>token</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.04</td>\n      <td>Jego</td>\n      <td>Jego wrze w temperaturze 100 stopni, a zamarza...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>Za</td>\n      <td>Za wrze w temperaturze 100 stopni, a zamarza w...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>Po</td>\n      <td>Po wrze w temperaturze 100 stopni, a zamarza w...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>Na</td>\n      <td>Na wrze w temperaturze 100 stopni, a zamarza w...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>W</td>\n      <td>W wrze w temperaturze 100 stopni, a zamarza w ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.09</td>\n      <td>Na</td>\n      <td>Na wrze w temperaturze 100 stopni, a zamarza w...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.05</td>\n      <td>We</td>\n      <td>We wrze w temperaturze 100 stopni, a zamarza w...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>Od</td>\n      <td>Od wrze w temperaturze 100 stopni, a zamarza w...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>W</td>\n      <td>W wrze w temperaturze 100 stopni, a zamarza w ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>we</td>\n      <td>we wrze w temperaturze 100 stopni, a zamarza w...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>xlm-roberta-base</td>\n      <td>0.11</td>\n      <td>Najlepiej</td>\n      <td>Najlepiej wrze w temperaturze 100 stopni, a za...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>xlm-roberta-base</td>\n      <td>0.10</td>\n      <td>-</td>\n      <td>- wrze w temperaturze 100 stopni, a zamarza w ...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>xlm-roberta-base</td>\n      <td>0.04</td>\n      <td>Najczęściej</td>\n      <td>Najczęściej wrze w temperaturze 100 stopni, a ...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>xlm-roberta-base</td>\n      <td>0.04</td>\n      <td>–</td>\n      <td>– wrze w temperaturze 100 stopni, a zamarza w ...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>xlm-roberta-base</td>\n      <td>0.03</td>\n      <td>Następnie</td>\n      <td>Następnie wrze w temperaturze 100 stopni, a za...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_for_all_models_and_return_pandas('[MASK] wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T21:17:16.646172800Z",
     "start_time": "2024-11-08T21:15:45.534517700Z"
    }
   },
   "id": "9410f9786823fef6"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the weights of TFDistilBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFXLMRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 model  score      token  \\\n0         bert-base-multilingual-cased   0.07          .   \n1         bert-base-multilingual-cased   0.06         to   \n2         bert-base-multilingual-cased   0.05         To   \n3         bert-base-multilingual-cased   0.04      Tonga   \n4         bert-base-multilingual-cased   0.04         Ta   \n5   distilbert-base-multilingual-cased   0.06     Miasto   \n6   distilbert-base-multilingual-cased   0.04    Obecnie   \n7   distilbert-base-multilingual-cased   0.03  popolasiù   \n8   distilbert-base-multilingual-cased   0.03      Grupa   \n9   distilbert-base-multilingual-cased   0.02       Jest   \n10                    xlm-roberta-base   0.14    Everest   \n11                    xlm-roberta-base   0.09      Egipt   \n12                    xlm-roberta-base   0.07      Nepal   \n13                    xlm-roberta-base   0.04      Malta   \n14                    xlm-roberta-base   0.03       Pisa   \n\n                                      sequence  \n0           . jest największą górą na świecie.  \n1          to jest największą górą na świecie.  \n2          To jest największą górą na świecie.  \n3       Tonga jest największą górą na świecie.  \n4          Ta jest największą górą na świecie.  \n5      Miasto jest największą górą na świecie.  \n6     Obecnie jest największą górą na świecie.  \n7   popolasiù jest największą górą na świecie.  \n8       Grupa jest największą górą na świecie.  \n9        Jest jest największą górą na świecie.  \n10    Everest jest największą górą na świecie.  \n11      Egipt jest największą górą na świecie.  \n12      Nepal jest największą górą na świecie.  \n13      Malta jest największą górą na świecie.  \n14       Pisa jest największą górą na świecie.  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score</th>\n      <th>token</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.07</td>\n      <td>.</td>\n      <td>. jest największą górą na świecie.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.06</td>\n      <td>to</td>\n      <td>to jest największą górą na świecie.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.05</td>\n      <td>To</td>\n      <td>To jest największą górą na świecie.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.04</td>\n      <td>Tonga</td>\n      <td>Tonga jest największą górą na świecie.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.04</td>\n      <td>Ta</td>\n      <td>Ta jest największą górą na świecie.</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.06</td>\n      <td>Miasto</td>\n      <td>Miasto jest największą górą na świecie.</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.04</td>\n      <td>Obecnie</td>\n      <td>Obecnie jest największą górą na świecie.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>popolasiù</td>\n      <td>popolasiù jest największą górą na świecie.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>Grupa</td>\n      <td>Grupa jest największą górą na świecie.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>Jest</td>\n      <td>Jest jest największą górą na świecie.</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>xlm-roberta-base</td>\n      <td>0.14</td>\n      <td>Everest</td>\n      <td>Everest jest największą górą na świecie.</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>xlm-roberta-base</td>\n      <td>0.09</td>\n      <td>Egipt</td>\n      <td>Egipt jest największą górą na świecie.</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>xlm-roberta-base</td>\n      <td>0.07</td>\n      <td>Nepal</td>\n      <td>Nepal jest największą górą na świecie.</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>xlm-roberta-base</td>\n      <td>0.04</td>\n      <td>Malta</td>\n      <td>Malta jest największą górą na świecie.</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>xlm-roberta-base</td>\n      <td>0.03</td>\n      <td>Pisa</td>\n      <td>Pisa jest największą górą na świecie.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_for_all_models_and_return_pandas('[MASK] jest największą górą na świecie.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T21:21:19.634829900Z",
     "start_time": "2024-11-08T21:18:19.738325800Z"
    }
   },
   "id": "2e7706d19b11a082"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the weights of TFDistilBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFXLMRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 model  score     token  \\\n0         bert-base-multilingual-cased   0.11         :   \n1         bert-base-multilingual-cased   0.03     linii   \n2         bert-base-multilingual-cased   0.03    stacji   \n3         bert-base-multilingual-cased   0.01     ulicy   \n4         bert-base-multilingual-cased   0.01      polu   \n5   distilbert-base-multilingual-cased   0.05     ziemi   \n6   distilbert-base-multilingual-cased   0.05         :   \n7   distilbert-base-multilingual-cased   0.04      ёсць   \n8   distilbert-base-multilingual-cased   0.03     Ziemi   \n9   distilbert-base-multilingual-cased   0.02   terenie   \n10                    xlm-roberta-base   0.14       ...   \n11                    xlm-roberta-base   0.13    drodze   \n12                    xlm-roberta-base   0.05       ...   \n13                    xlm-roberta-base   0.05  wolności   \n14                    xlm-roberta-base   0.04   mieście   \n\n                        sequence  \n0          Samochody jeżdżą na :  \n1      Samochody jeżdżą na linii  \n2     Samochody jeżdżą na stacji  \n3      Samochody jeżdżą na ulicy  \n4       Samochody jeżdżą na polu  \n5      Samochody jeżdżą na ziemi  \n6          Samochody jeżdżą na :  \n7       Samochody jeżdżą na ёсць  \n8      Samochody jeżdżą na Ziemi  \n9    Samochody jeżdżą na terenie  \n10        Samochody jeżdżą na...  \n11    Samochody jeżdżą na drodze  \n12       Samochody jeżdżą na ...  \n13  Samochody jeżdżą na wolności  \n14   Samochody jeżdżą na mieście  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score</th>\n      <th>token</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.11</td>\n      <td>:</td>\n      <td>Samochody jeżdżą na :</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>linii</td>\n      <td>Samochody jeżdżą na linii</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>stacji</td>\n      <td>Samochody jeżdżą na stacji</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.01</td>\n      <td>ulicy</td>\n      <td>Samochody jeżdżą na ulicy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.01</td>\n      <td>polu</td>\n      <td>Samochody jeżdżą na polu</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.05</td>\n      <td>ziemi</td>\n      <td>Samochody jeżdżą na ziemi</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.05</td>\n      <td>:</td>\n      <td>Samochody jeżdżą na :</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.04</td>\n      <td>ёсць</td>\n      <td>Samochody jeżdżą na ёсць</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>Ziemi</td>\n      <td>Samochody jeżdżą na Ziemi</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>terenie</td>\n      <td>Samochody jeżdżą na terenie</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>xlm-roberta-base</td>\n      <td>0.14</td>\n      <td>...</td>\n      <td>Samochody jeżdżą na...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>xlm-roberta-base</td>\n      <td>0.13</td>\n      <td>drodze</td>\n      <td>Samochody jeżdżą na drodze</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>xlm-roberta-base</td>\n      <td>0.05</td>\n      <td>...</td>\n      <td>Samochody jeżdżą na ...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>xlm-roberta-base</td>\n      <td>0.05</td>\n      <td>wolności</td>\n      <td>Samochody jeżdżą na wolności</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>xlm-roberta-base</td>\n      <td>0.04</td>\n      <td>mieście</td>\n      <td>Samochody jeżdżą na mieście</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_for_all_models_and_return_pandas('Samochody jeżdżą na [MASK]')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T21:24:10.607850400Z",
     "start_time": "2024-11-08T21:22:10.563088200Z"
    }
   },
   "id": "ccd660af2e9905a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Check zero-shot learning capabilites of the models. Provide at least 5 sentences with different sentiment for the following scheme: \"'Ten film to był kiler. Nie mogłem się oderwać od ekranu.' Wypowiedź ta ma jest zdecydowanie `[MASK]`\" Try different prompts, to see if they make any difference."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "774083b5078f220b"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the weights of TFDistilBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFXLMRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 model  score      token  \\\n0         bert-base-multilingual-cased   0.10      słowa   \n1         bert-base-multilingual-cased   0.05      znana   \n2         bert-base-multilingual-cased   0.03         ta   \n3         bert-base-multilingual-cased   0.02       sama   \n4         bert-base-multilingual-cased   0.02         to   \n5   distilbert-base-multilingual-cased   0.24      filmu   \n6   distilbert-base-multilingual-cased   0.09      znana   \n7   distilbert-base-multilingual-cased   0.04    serialu   \n8   distilbert-base-multilingual-cased   0.02   Tuttavia   \n9   distilbert-base-multilingual-cased   0.01     Dakota   \n10                    xlm-roberta-base   0.28  najlepsza   \n11                    xlm-roberta-base   0.17      dobra   \n12                    xlm-roberta-base   0.04     piękna   \n13                    xlm-roberta-base   0.03      ważna   \n14                    xlm-roberta-base   0.02       taka   \n\n                                             sequence  \n0   \" Ten film to był kiler. Nie mogłem się oderwa...  \n1   \" Ten film to był kiler. Nie mogłem się oderwa...  \n2   \" Ten film to był kiler. Nie mogłem się oderwa...  \n3   \" Ten film to był kiler. Nie mogłem się oderwa...  \n4   \" Ten film to był kiler. Nie mogłem się oderwa...  \n5   \" Ten film to był kiler. Nie mogłem się oderwa...  \n6   \" Ten film to był kiler. Nie mogłem się oderwa...  \n7   \" Ten film to był kiler. Nie mogłem się oderwa...  \n8   \" Ten film to był kiler. Nie mogłem się oderwa...  \n9   \" Ten film to był kiler. Nie mogłem się oderwa...  \n10  \"Ten film to był kiler. Nie mogłem się oderwać...  \n11  \"Ten film to był kiler. Nie mogłem się oderwać...  \n12  \"Ten film to był kiler. Nie mogłem się oderwać...  \n13  \"Ten film to był kiler. Nie mogłem się oderwać...  \n14  \"Ten film to był kiler. Nie mogłem się oderwać...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score</th>\n      <th>token</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.10</td>\n      <td>słowa</td>\n      <td>\" Ten film to był kiler. Nie mogłem się oderwa...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.05</td>\n      <td>znana</td>\n      <td>\" Ten film to był kiler. Nie mogłem się oderwa...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>ta</td>\n      <td>\" Ten film to był kiler. Nie mogłem się oderwa...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>sama</td>\n      <td>\" Ten film to był kiler. Nie mogłem się oderwa...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>to</td>\n      <td>\" Ten film to był kiler. Nie mogłem się oderwa...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.24</td>\n      <td>filmu</td>\n      <td>\" Ten film to był kiler. Nie mogłem się oderwa...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.09</td>\n      <td>znana</td>\n      <td>\" Ten film to był kiler. Nie mogłem się oderwa...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.04</td>\n      <td>serialu</td>\n      <td>\" Ten film to był kiler. Nie mogłem się oderwa...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>Tuttavia</td>\n      <td>\" Ten film to był kiler. Nie mogłem się oderwa...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.01</td>\n      <td>Dakota</td>\n      <td>\" Ten film to był kiler. Nie mogłem się oderwa...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>xlm-roberta-base</td>\n      <td>0.28</td>\n      <td>najlepsza</td>\n      <td>\"Ten film to był kiler. Nie mogłem się oderwać...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>xlm-roberta-base</td>\n      <td>0.17</td>\n      <td>dobra</td>\n      <td>\"Ten film to był kiler. Nie mogłem się oderwać...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>xlm-roberta-base</td>\n      <td>0.04</td>\n      <td>piękna</td>\n      <td>\"Ten film to był kiler. Nie mogłem się oderwać...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>xlm-roberta-base</td>\n      <td>0.03</td>\n      <td>ważna</td>\n      <td>\"Ten film to był kiler. Nie mogłem się oderwać...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>xlm-roberta-base</td>\n      <td>0.02</td>\n      <td>taka</td>\n      <td>\"Ten film to był kiler. Nie mogłem się oderwać...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_for_all_models_and_return_pandas('\"Ten film to był kiler. Nie mogłem się oderwać od ekranu.\" Wypowiedź ta jest zdecydowanie [MASK].')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T21:27:17.522364100Z",
     "start_time": "2024-11-08T21:25:54.931786800Z"
    }
   },
   "id": "56304dfc989bfdaf"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the weights of TFDistilBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFXLMRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 model  score       token  \\\n0         bert-base-multilingual-cased   0.09           \"   \n1         bert-base-multilingual-cased   0.02           w   \n2         bert-base-multilingual-cased   0.02          pt   \n3         bert-base-multilingual-cased   0.02         tzw   \n4         bert-base-multilingual-cased   0.02      tekstu   \n5   distilbert-base-multilingual-cased   0.17           勅   \n6   distilbert-base-multilingual-cased   0.08       pracy   \n7   distilbert-base-multilingual-cased   0.04      władzy   \n8   distilbert-base-multilingual-cased   0.04   człowieka   \n9   distilbert-base-multilingual-cased   0.02       prawa   \n10                    xlm-roberta-base   0.24           .   \n11                    xlm-roberta-base   0.08   pozytywny   \n12                    xlm-roberta-base   0.04         ...   \n13                    xlm-roberta-base   0.03  wewnętrzny   \n14                    xlm-roberta-base   0.02        ować   \n\n                                             sequence  \n0   \" Nienawidzę polityki. \" Zdanie to ma wydźwięk \".  \n1   \" Nienawidzę polityki. \" Zdanie to ma wydźwięk w.  \n2   \" Nienawidzę polityki. \" Zdanie to ma wydźwięk...  \n3   \" Nienawidzę polityki. \" Zdanie to ma wydźwięk...  \n4   \" Nienawidzę polityki. \" Zdanie to ma wydźwięk...  \n5   \" Nienawidzę polityki. \" Zdanie to ma wydźwięk 勅.  \n6   \" Nienawidzę polityki. \" Zdanie to ma wydźwięk...  \n7   \" Nienawidzę polityki. \" Zdanie to ma wydźwięk...  \n8   \" Nienawidzę polityki. \" Zdanie to ma wydźwięk...  \n9   \" Nienawidzę polityki. \" Zdanie to ma wydźwięk...  \n10    \"Nienawidzę polityki.\" Zdanie to ma wydźwięk. .  \n11  \"Nienawidzę polityki.\" Zdanie to ma wydźwięk p...  \n12  \"Nienawidzę polityki.\" Zdanie to ma wydźwięk... .  \n13  \"Nienawidzę polityki.\" Zdanie to ma wydźwięk w...  \n14  \"Nienawidzę polityki.\" Zdanie to ma wydźwiękow...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score</th>\n      <th>token</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.09</td>\n      <td>\"</td>\n      <td>\" Nienawidzę polityki. \" Zdanie to ma wydźwięk \".</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>w</td>\n      <td>\" Nienawidzę polityki. \" Zdanie to ma wydźwięk w.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>pt</td>\n      <td>\" Nienawidzę polityki. \" Zdanie to ma wydźwięk...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>tzw</td>\n      <td>\" Nienawidzę polityki. \" Zdanie to ma wydźwięk...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>tekstu</td>\n      <td>\" Nienawidzę polityki. \" Zdanie to ma wydźwięk...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.17</td>\n      <td>勅</td>\n      <td>\" Nienawidzę polityki. \" Zdanie to ma wydźwięk 勅.</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.08</td>\n      <td>pracy</td>\n      <td>\" Nienawidzę polityki. \" Zdanie to ma wydźwięk...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.04</td>\n      <td>władzy</td>\n      <td>\" Nienawidzę polityki. \" Zdanie to ma wydźwięk...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.04</td>\n      <td>człowieka</td>\n      <td>\" Nienawidzę polityki. \" Zdanie to ma wydźwięk...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>prawa</td>\n      <td>\" Nienawidzę polityki. \" Zdanie to ma wydźwięk...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>xlm-roberta-base</td>\n      <td>0.24</td>\n      <td>.</td>\n      <td>\"Nienawidzę polityki.\" Zdanie to ma wydźwięk. .</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>xlm-roberta-base</td>\n      <td>0.08</td>\n      <td>pozytywny</td>\n      <td>\"Nienawidzę polityki.\" Zdanie to ma wydźwięk p...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>xlm-roberta-base</td>\n      <td>0.04</td>\n      <td>...</td>\n      <td>\"Nienawidzę polityki.\" Zdanie to ma wydźwięk... .</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>xlm-roberta-base</td>\n      <td>0.03</td>\n      <td>wewnętrzny</td>\n      <td>\"Nienawidzę polityki.\" Zdanie to ma wydźwięk w...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>xlm-roberta-base</td>\n      <td>0.02</td>\n      <td>ować</td>\n      <td>\"Nienawidzę polityki.\" Zdanie to ma wydźwiękow...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_for_all_models_and_return_pandas('\"Nienawidzę polityki.\" Zdanie to ma wydźwięk [MASK].')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T21:29:25.291837Z",
     "start_time": "2024-11-08T21:28:01.521692400Z"
    }
   },
   "id": "4474409beb640937"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the weights of TFDistilBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFXLMRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 model  score      token  \\\n0         bert-base-multilingual-cased   0.08      słowa   \n1         bert-base-multilingual-cased   0.03      znana   \n2         bert-base-multilingual-cased   0.02        tzw   \n3         bert-base-multilingual-cased   0.02      stała   \n4         bert-base-multilingual-cased   0.02          \"   \n5   distilbert-base-multilingual-cased   0.07      znana   \n6   distilbert-base-multilingual-cased   0.02        nie   \n7   distilbert-base-multilingual-cased   0.02    dopiero   \n8   distilbert-base-multilingual-cased   0.02      pracy   \n9   distilbert-base-multilingual-cased   0.02  człowieka   \n10                    xlm-roberta-base   0.13  najlepsza   \n11                    xlm-roberta-base   0.12      dobra   \n12                    xlm-roberta-base   0.07       moja   \n13                    xlm-roberta-base   0.06      ważna   \n14                    xlm-roberta-base   0.03     piękna   \n\n                                             sequence  \n0   \" Staram się być obiektywyny niezależnie co si...  \n1   \" Staram się być obiektywyny niezależnie co si...  \n2   \" Staram się być obiektywyny niezależnie co si...  \n3   \" Staram się być obiektywyny niezależnie co si...  \n4   \" Staram się być obiektywyny niezależnie co si...  \n5   \" Staram się być obiektywyny niezależnie co si...  \n6   \" Staram się być obiektywyny niezależnie co si...  \n7   \" Staram się być obiektywyny niezależnie co si...  \n8   \" Staram się być obiektywyny niezależnie co si...  \n9   \" Staram się być obiektywyny niezależnie co si...  \n10  \"Staram się być obiektywyny niezależnie co się...  \n11  \"Staram się być obiektywyny niezależnie co się...  \n12  \"Staram się być obiektywyny niezależnie co się...  \n13  \"Staram się być obiektywyny niezależnie co się...  \n14  \"Staram się być obiektywyny niezależnie co się...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score</th>\n      <th>token</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.08</td>\n      <td>słowa</td>\n      <td>\" Staram się być obiektywyny niezależnie co si...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.03</td>\n      <td>znana</td>\n      <td>\" Staram się być obiektywyny niezależnie co si...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>tzw</td>\n      <td>\" Staram się być obiektywyny niezależnie co si...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>stała</td>\n      <td>\" Staram się być obiektywyny niezależnie co si...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>\"</td>\n      <td>\" Staram się być obiektywyny niezależnie co si...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.07</td>\n      <td>znana</td>\n      <td>\" Staram się być obiektywyny niezależnie co si...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>nie</td>\n      <td>\" Staram się być obiektywyny niezależnie co si...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>dopiero</td>\n      <td>\" Staram się być obiektywyny niezależnie co si...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>pracy</td>\n      <td>\" Staram się być obiektywyny niezależnie co si...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>distilbert-base-multilingual-cased</td>\n      <td>0.02</td>\n      <td>człowieka</td>\n      <td>\" Staram się być obiektywyny niezależnie co si...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>xlm-roberta-base</td>\n      <td>0.13</td>\n      <td>najlepsza</td>\n      <td>\"Staram się być obiektywyny niezależnie co się...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>xlm-roberta-base</td>\n      <td>0.12</td>\n      <td>dobra</td>\n      <td>\"Staram się być obiektywyny niezależnie co się...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>xlm-roberta-base</td>\n      <td>0.07</td>\n      <td>moja</td>\n      <td>\"Staram się być obiektywyny niezależnie co się...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>xlm-roberta-base</td>\n      <td>0.06</td>\n      <td>ważna</td>\n      <td>\"Staram się być obiektywyny niezależnie co się...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>xlm-roberta-base</td>\n      <td>0.03</td>\n      <td>piękna</td>\n      <td>\"Staram się być obiektywyny niezależnie co się...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_for_all_models_and_return_pandas('\"Staram się być obiektywyny niezależnie co się dzieje.\" Wypowiedź ta jest zdecydowanie [MASK].')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T21:32:34.371723500Z",
     "start_time": "2024-11-08T21:30:20.386439500Z"
    }
   },
   "id": "3a7c20747194ac6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. Take into accout the fact, that causal language models such as PapuGaPT2 or plT5, will only generate continuations of the sentenes, so the examples have to be created according to that paradigm."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2e35fba5a53de60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. Answer the following questions (2 points):\n",
    "\n",
    "**Which of the models produced the best results?**\n",
    "The xlm-roberta-base model produced the most consistent results across tasks. It performed relatively well in recognizing Polish grammar, capturing some long-distance dependencies, and demonstrated a limited amount of general knowledge.\n",
    "\n",
    "**Was any of the models able to capture Polish grammar?**\n",
    "All of the models showed some ability to recognize Polish grammar, especially basic grammatical. However, they often struggled with more complex grammatical structures, especially in long sentences or cases requiring consistency in gender and form.\n",
    "\n",
    "**Was any of the models able to capture long-distant relationships between the words?**\n",
    "Each model occasionally managed to capture long-distance relationships, but xlm-roberta-base was the best in this area. It was more likely to maintain grammatical consistency over longer distances, though it still made mistakes with gender and agreement when dealing with more complex sentences.\n",
    "\n",
    "**Was any of the models able to capture world knowledge?**\n",
    "Xlm-roberta-base showed some understanding of general world knowledge, like identifying Mount Everest as the tallest mountain. However, all models had trouble with common knowledge, such as identifying water as the substance that boils at 100°C or gasoline as the fuel for cars. This suggests that the models' real-world knowledge is limited.\n",
    "\n",
    "**Was any of the models good at doing zero-shot classification?**\n",
    "Xlm-roberta-base performed moderately well in zero-shot classification but struggled with sentiment tasks.\n",
    "\n",
    "**What are the most striking errors made by the models?**\n",
    "The models made several notable errors, such as predicting irrelevant tokens like punctuation or characters (e.g., 勅) and producing incomplete fragments (e.g., \"##dzi\") that didn’t fit the context. They often struggled with grammar consistency, failing to maintain gender or case agreement in longer sentences. Additionally, they showed a lack of basic world knowledge, missing obvious facts like “water” boiling at 100°C or “gasoline” being a fuel for cars."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e363daed63a46ad9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hints\n",
    "\n",
    "1. Language modelling (LM) is a task concentrated on computing the probability distribution of words given a sequence of\n",
    "   preceding words.\n",
    "1. In the past the most popular LM were based on n-gram counting. The distribution of probability of the next word was\n",
    "   approximated by the relative frequency of the last n-words, preceding this word. Usually n=3, since larger values\n",
    "   resulted in extremely large datasets.\n",
    "1. Many algorithms were devised to improve that probability estimates for infrequent words. Among them Kneser-Ney was\n",
    "   the most popular.\n",
    "1. SRI LM is the most popular toolkit for creating traditional language models.\n",
    "1. At present recurrent neural networks, attention networks and transformers are the most popular neural-network\n",
    "   architectures for creating LMs.\n",
    "1. The largest LM currently is GPT-3 described in (mind the number of authors!) *Language Models are Few-Shot Learners*\n",
    "   Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\n",
    "   Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon\n",
    "   Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\n",
    "   Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\n",
    "   Sutskever, Dario Amodei"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41b71b6d9ab9ae77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c47d72153721209e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
